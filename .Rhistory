library(reshape2)
library(caTools)
library(corrplot)
library(MLmetrics)
library(ggfortify)
library(pROC)
set.seed(123)
data_0 <- read.csv("bmd.csv", stringsAsFactors = TRUE) #converting all strings to factors
str(data_0)
data_0 <- read.csv("bmd.csv", stringsAsFactors = TRUE) #converting all strings to factors
data_0 <- read.csv("bmd.csv", stringsAsFactors = TRUE) #converting all strings to factors
str(data_0)
summary(data_0)
#little bit of domain knowledge usage
data_0$bmi <- data_0$weight_kg/(data_0$height_cm/100)^2
sample <- sample.split(data_0$fracture,.7)
train <- data_0 %>% filter(sample==TRUE)
test <- data_0 %>% filter(sample==FALSE)
#verifying the dimensions
dim(test) #test set
dim(train) #train set
model1 <- glm(fracture~age+sex+bmi+bmd+medication+waiting_time, data = train,family = binomial())
test$predicted_probs <- predict(model1,test,type = "response")
test$predicted_frac <- ifelse(test$predicted_probs>0.5,"no fracture","fracture")
#Confusion Matrix
ConfusionMatrix(test$predicted_frac,test$fracture)
#Accuracy
Accuracy(test$predicted_frac,test$fracture)
#Precision
Precision(test$fracture,test$predicted_frac,positive = "fracture")
#Recall
Recall(test$fracture,test$predicted_frac,positive = "fracture")
#auc-roc curve
roc(test$fracture ~ predicted_probs, plot=TRUE,print.auc=TRUE,data = test,legacy.axes=TRUE)
data <- read.csv("bank-full.csv",sep = ";")
View(data)
data <- read.csv("titanic.csv")
View(data)
data <- read.csv("titanic.csv",na.strings = c(" "))
View(data)
data <- read.csv("titanic.csv",na.strings = c(" ","","NA"))
nrow(data)
ncol(data)
data['Age']
data[['Age',"Pclass"]]
data[c('Age',"Pclass")]
data[2:5,]
data[2:5,"Age"]
data[2:5,c("Age","Pclass")]
ifelse(5>1,"True","F")
library(tidyverse)
data <- read.csv("bank-full.csv")#,sep = ";")
View(data)
data <- read.csv("bank-full.csv",sep = ";")
View(data)
head(data)
tail(data)
str(data)
ndim(data)
dim(data)
summary(data)
names(data)
data$age
data["age"]
head(data[c("age","job")])
head(data[4:5])
head(data[4:8])
data1 <- data[!"age"]
data1 <- data[-1]
data1 <- data1 %>% rename(JOB=job)
data1 <- data1 %>% rename(JOB=job,
significance=education)
data <- read.csv("bank-full.csv",sep = ";")
data1 <- data[-1]
data1 <- data1 %>% rename(JOB=job,
significance=education)
data2 <-  data1 %>% arrange(age)
data2 <-  data %>% arrange(age)
View(data2)
data2 <-  data %>% arrange(c(age,day))
data2 <-  data %>% arrange(c("age","day"))
data2 <-  data %>% arrange(age,day)
View(data2)
head(data2 %>% filter(job=="technician"))
table(data2$poutcome)
data2$pred <- ifelse(data2$y=="yes",1,0)
head(data2[c("y","pred")],15)
data3 <- data2 %>% group_by(marital) %>% summarise(mean=mean(balance))
View(data3)
data3 <- data2 %>% group_by(marital) %>% summarise(min=min(balance))
colSums(is.na(data2))
summary(data)
data <- read.csv("bank-full.csv",sep = ";",stringsAsFactors = TRUE)
summary(data)
data %>% distinct(job)
data <- read.csv("Pokemon.csv")
View(data)
data <- read.csv("Pokemon.csv",na.strings = c(""," ","NA"))
View(data)
head(data)
tail(data)
str(data)
summary(data)
colSums(is.na(data))
colSums(is.na(data))/nrow(data)
data <- data %>% select(!"Type.2")
names(data)
data <- read.csv("dplyr.csv")
View(data)
ny_data <- read.csv("dplyr.csv")
data1 <- my_data %>% distinct()
my_data <- read.csv("dplyr.csv")
data1 <- my_data %>% distinct()
data1 <- my_data %>% distinct(Index)
View(data1)
data1 <- my_data %>% distinct(Index,.keep_all = TRUE)
data2 <- my_data %>% select(!starts_with("y"))
View(data2)
View(my_data)
data3 <- my_data %>% select(contains("I"))
View(data3)
data4 <- my_data %>% select(Index in c("A","C"))
data4 <- my_data %>% select(Index %in% c("A","C"))
data4 <- my_data %>% select(Index %in% c("A","C"))
data4 <- my_data %>% filter(Index %in% c("A","C"))
View(data4)
data5 <- my_data %>% filter(!Index %in% c("A","C"))
View(data5)
View(data5)
my_data %>% summarise(mean=mean(Y2015),sum=sum(Y2015))
my_data %>% group_by(Index) %>%
summarise(mean=mean(Y2011),median=median(Y2011))
my_data %>% group_by(Index) %>% summarise(mean=mean(Y2011),median=median(Y2011))
my_data %>% sample_frac(.1)
my_data %>% sample_frac(.1)
data <- read.csv("dplyr.csv")
data <- read.csv("titanic.csv")
View(data)
data <- read.csv("titanic.csv", na.strings = c(""," ","NA"))
View(data)
colSums(is.na(data))
mean(colSums(is.na(data)))
colSums(is.na(data))/nrow(data)
View(data)
df <- read.csv("dplyr.csv")
View(df)
library(tidyverse)
df %>% sample_n(5)
df %>% sample_n(5)
df %>% sample_n(5)
df %>% sample_frac(.5)
df %>% sample_frac(.05)
library(tidyverse)
df <- read.csv("insurance.csv")
df <- read.csv("titanic.csv")
head(df)
tail(df)
str(df)
dim(df)
summary(df)
null_cols <- colSums(is.na(df))/nrow(df)
null_cols[null_cols!=0]
names(df)
# unique rows()
df1 <- df %>% distinct()
View(df)
df <- read.csv("titanic.csv",na.strings = c(""," ","NA"))
# unique rows()
df1 <- df %>% distinct(Pclass)
View(df1)
# unique rows()
df1 <- df %>% distinct(Pclass,.keep_all = TRUE)
View(df1)
df[["Pclass","Sex"]]
df[c("Pclass","Sex")]
df[c("Pclass","Sex","Fare")]
df %>% select(Pclass,Sex,Fare)
df %>% select(Pclass,Sex,Fare,Age)
df <- read.csv("insurance.csv",na.strings = c(""," ","NA"))
head(df)
tail(df)
str(df)
dim(df)
summary(df)
null_cols <- colSums(is.na(df))/nrow(df)
null_cols[null_cols!=0]
names(df)
# unique rows()
df1 <- df %>% distinct(Pclass,.keep_all = TRUE)
# unique rows()
df1 <- df %>% distinct()#Pclass,.keep_all = TRUE)
df["Fare"]
df["age"]
df$age
which(!grepl("factor",sapply(df,class)))
df <- read.csv("insurance.csv",na.strings = c(""," ","NA"),
stringsAsFactors = TRUE)
null_cols <- colSums(is.na(df))/nrow(df)
null_cols[null_cols!=0]
names(df)
which(!grepl("factor",sapply(df,class)))
box_hist <- function(df,col){
print(df %>% ggplot(aes(aes_string(col)))+geom_boxplot())
print(df %>% ggplot(aes(aes_string(col)))+geom_histogram())
}
box_hist(df,"age")
box_hist <- function(df,col){
print(df %>% ggplot(aes_string(col))+geom_boxplot())
print(df %>% ggplot(aes_string(col))+geom_histogram())
}
box_hist(df,"age")
box_hist <- function(df,col){
print(df %>% ggplot(aes_string(col))+geom_boxplot())
print(df %>% ggplot(aes_string(col))+geom_histogram(aes(y=..density..))+geom_density())
}
box_hist(df,"age")
box_hist <- function(df,col){
print(df %>% ggplot(aes_string(col))+geom_boxplot())
print(df %>% ggplot(aes_string(col))+geom_density())
}
box_hist(df,"age")
box_hist(df,"age")
box_hist <- function(df,col){
print(df %>% ggplot(aes_string(col))+geom_boxplot())
print(df %>% ggplot(aes_string(col))+geom_histogram(aes(y=..density..))+geom_density())
}
box_hist(df,"age")
linearity <- function(df,a,b){
print(df %>% ggplot(aes_string(a,b))+geom_point()+geom_smooth(method="lm"))
print(cor(df$a,df$b))
}
linearity(df,"age","charges")
df %>% ggplot(aes(age))+geom_histogram(aes(fill=sex),position = "dodge")
df <- read.csv("insurance.csv",na.strings = c(""," ","NA"),
stringsAsFactors = TRUE)
sapply(df, class)
which(!grepl("factor",sapply(df,class)))
idx <- which(!grepl("factor",sapply(df,class)))
df[idx]
df %>% rename("AGE"=age,"BMI"="bmi")
df %>% rename("AGE"=age,"BMI"="bmi")
df2 <- df %>% rename("AGE"=age,"BMI"="bmi")
library(tidyverse)
df %>% rename("AGE"=age,"BMI"="bmi")
df$age+2
df[c("age","bmi")]
head(df[c("age","bmi")])
head(df[c("age","bmi")]+1)
df %>% select(region %in% c("southeast","southwest"))
df %>% select(region =="southeast"|region=="southwest"))
df %>% select(region =="southeast"|region=="southwest")
df %>% filter(region %in% c("southeast","southwest"))
length(df %>% filter(region %in% c("southeast","southwest")))
nrow(df %>% filter(region %in% c("southeast","southwest")))
df %>% arrange(age,bmi)
head(df %>% arrange(age,bmi))
type(df$age)
typeof(df$age)
typeof(df["age"])
typeof([df["age"]])
typeof(df[["age"]])
library(tidyverse)
data <- read.csv("bank-full.csv",sep = ";",stringsAsFactors = TRUE)
sapply(data, class)
table(data$y)
library(fastDummies)
factor_idx <- which(grepl("factor",sapply(data, class)))
dummy_data <- dummy_cols(data,select_columns = names(data)[factor_idx[-10]],remove_first_dummy = TRUE
,remove_selected_columns = TRUE)
#train-test split
library(caTools)
names(dummy_data)[c(9,14)] <- c("job_blue_collar","job_self_employed")
sample <- sample.split(dummy_data$y,.7)
train <- dummy_data[sample,]
test <- dummy_data[!sample,]
library(smotefamily)
smote_data <- smote_results$data
smote_results <- SMOTE(train[-8],train$y)
smote_data <- smote_results$data
smote_data$class <- as.factor(smote_data$class)
table(smote_data$class)
library(randomForest)
rf <- randomForest(class~.,smote_data,ntree=300)
predictions_test <- predict(rf,test)
predictions_train <- predict(rf,smote_data[-43])
library(MLmetrics)
ConfusionMatrix(predictions_test,test$y)
Accuracy(predictions_test,test$y)
Accuracy(predictions_train,smote_data$class)
Recall(test$y,predictions_test,positive = "yes")
Precision(test$y,predictions_test,positive = "yes")
rf$confusion
plot(rf)
rf <- randomForest(class~.,smote_data)
predictions_test <- predict(rf,test)
predictions_train <- predict(rf,smote_data[-43])
ConfusionMatrix(predictions_test,test$y)
Accuracy(predictions_test,test$y)
Accuracy(predictions_train,smote_data$class)
Recall(test$y,predictions_test,positive = "yes")
Precision(test$y,predictions_test,positive = "yes")
rf$confusion
plot(rf)
t <- tuneRF(smote_data[-43],smote_data$class, ntreeTry= 200,
trace = TRUE,plot = TRUE)
trf <- randomForest(class~.,smote_data,ntree=200,mtry=12)
tpredictions_test <- predict(trf,test)
tpredictions_train <- predict(trf,smote_data[-43])
ConfusionMatrix(tpredictions_test,test$y)
Accuracy(tpredictions_test,test$y)
Accuracy(tpredictions_train,smote_data$class)
Recall(test$y,tpredictions_test,positive = "yes")
Precision(test$y,tpredictions_test,positive = "yes")
varImpPlot(trf)
head(data)
summary(data)
table(data$pdays)
table(data$pdays+1)
install.packages(c("arules","arulesViz"))
#apriori algorithm
library("arules")
library(arulesViz)
df <- read.csv("Groceries_Data.csv")
View(df)
colSums(is.na(df))
pattern <- apriori(df,parameter = list(support=0.02,confidence=0.05))
pattern <- apriori(df[-1],parameter = list(support=0.02,confidence=0.05))
ruleExplorer(pattern)
ruleExplorer(pattern)
#install.packages("tm") #text mining
library("tm")
#install.packages("SnowballC") #
library("SnowballC")
#install.packages("wordcloud2")
library("wordcloud2")
#install.packages("RColorBrewer")
library("RColorBrewer")
text=readLines("textdata.txt")
docs=Corpus(VectorSource(text)) #cnverting data to corpus
View(docs)
docs=tm_map(docs,content_transformer(tolower)) #
docs=tm_map(docs,removeNumbers)
docs=tm_map(docs,removeWords,stopwords("english"))
docs[0]
###
docs=tm_map(docs,removePunctuation)
docs=tm_map(docs,stripWhitespace)
dtm=TermDocumentMatrix(docs)
View(dtm)
v=sort(rowSums(m),decreasing = TRUE)
d=data.frame(word=names(v),freq=v)
head(d,10)
set.seed(1234)
#wordcloud2(words=d$word,freq =d$freq,min.freq = 1,max.words = 200,random.order = FALSE,rot.per = 0.35,colors=brewer.pal(8,"Dark2"))
wordcloud2(d,shape="rectangle")
d=data.frame(word=names(v),freq=v)
v=sort(rowSums(m),decreasing = TRUE)
m=as.matrix(dtm)
v=sort(rowSums(m),decreasing = TRUE)
d=data.frame(word=names(v),freq=v)
head(d,10)
set.seed(1234)
#wordcloud2(words=d$word,freq =d$freq,min.freq = 1,max.words = 200,random.order = FALSE,rot.per = 0.35,colors=brewer.pal(8,"Dark2"))
wordcloud2(d,shape="rectangle")
# Telecommunications customer churn data
churn_df <- read.csv('churn_data.csv')
setwd("C:/Users/91903/Desktop/R/Rf-hype")
# Telecommunications customer churn data
churn_df <- read.csv('churn_data.csv')
set.seed(314)
churn_split <- initial_split(churn_df, prop = 0.75,
strata = canceled_service)
library(tidymodels)
library(tidyverse)
# Telecommunications customer churn data
churn_df <- read.csv('churn_data.csv')
churn_split <- initial_split(churn_df, prop = 0.75,
strata = canceled_service)
churn_training <- churn_split %>% training()
churn_test <- churn_split %>% testing()
churn_folds <- vfold_cv(churn_training, v = 5)
churn_recipe <- recipe(canceled_service ~ ., data = churn_training) %>%
step_YeoJohnson(all_numeric(), -all_outcomes()) %>%
step_normalize(all_numeric(), -all_outcomes()) %>%
step_dummy(all_nominal(), -all_outcomes())
churn_recipe %>%
prep() %>%
bake(new_data = churn_training)
rf_model <- rand_forest(mtry = tune(),
trees = tune(),
min_n = tune()) %>%
set_engine('ranger', importance = "impurity") %>%
set_mode('classification')
rf_workflow <- workflow() %>%
add_model(rf_model) %>%
add_recipe(churn_recipe)
set.seed(314)
rf_grid <- grid_random(mtry() %>% range_set(c(4, 12)),
trees(),
min_n(),
size = 10)
rf_grid
## Tune random forest workflow
set.seed(314)
rf_tuning <- rf_workflow %>%
tune_grid(resamples = churn_folds,
grid = rf_grid)
# Show the top 5 best models based on roc_auc metric
rf_tuning %>% show_best('roc_auc')
# importing necessary libraries
library(tidyverse)
library(randomForest)
library(MLmetrics)
library(caTools)
#importing the dataset
df <- read.csv("Computer_Data.csv",stringsAsFactors = TRUE)
setwd("C:/Users/91903/Desktop/Deploy/R-Deploy")
#importing the dataset
df <- read.csv("Computer_Data.csv",stringsAsFactors = TRUE)
summary(df)
#dropping list column
df <- df[-1]
#train test split
sample <- sample.split(df$price,.7)
train <- df[sample,]
test <- df[!sample,]
#modelling with default parameters
model1 <- randomForest(price~.,data = train)
predictions <- predict(model1,test[-1])
plot(model1)
RMSE(predictions,test$price)
MAPE(predictions,test$price)
colSums(is.na(df))
model2 <- tuneRF(train[,-1],train[,1],ntreeTry = 300, trace = TRUE, plot = TRUE)
test[2,]
test[1,]
test[3,]
test[2,]
temp <- data.frame(speed=as.integer(25),hd=as.integer(170),ram=as.integer(4),screen=as.integer(15),cd=c("no"),multi=c("no"),premium=c("yes"),ads=as.integer(94),trend=as.integer(1), stringsAsFactors = TRUE)
model1$forest$xlevels$cd
levels(temp$cd) <- model1$forest$xlevels$cd
levels(temp$multi) <- model1$forest$xlevels$multi
levels(temp$premium) <- model1$forest$xlevels$premium
tmodel <- randomForest(price~.,data = train, mtry = 6, ntree =200)
tmodel <- randomForest(price~.,data = train, mtry = 6, ntree =300)
predictions <- predict(tmodel,temp)
predictions <- predict(tmodel,test)
MAPE(predictions,test[,1])*100
RMSE(train[,1],predict(tmodel,train[,-1]))
RMSE(test[,1],predict(tmodel,test[,-1]))
distinct(df$ads)
table(df$ads)
table(df$ads)[10]
table(df$ads)
test[2,]
predictions <- predict(tmodel,temp)
library(randomForest)
library(MLmetrics)
library(caTools)
# importing necessary libraries
library(tidyverse)
print(tmodel)
summary(df)
MAE(predictions,test[,1])*100
RMSE(train[,1],predict(tmodel,train[,-1]))
RMSE(test[,1],predict(tmodel,test[,-1]))
MSE(predictions,test[,1])*100
MAE(predictions,test[,1])
MAPE(predictions,test[,1])*100
predictions <- predict(tmodel,temp)
MAE(predictions,test[,1])
MAPE(predictions,test[,1])*100
#importing the dataset
df <- read.csv("Computer_Data.csv",stringsAsFactors = TRUE)
summary(df)
#dropping list column
df <- df[-1]
#train test split
sample <- sample.split(df$price,.7)
train <- df[sample,]
test <- df[!sample,]
#modelling with default parameters
model1 <- randomForest(price~.,data = train)
plot(model1)
MAE(predictions,test[,1])
predictions <- predict(model1,test[-1])
MAE(predictions,test[,1])
tmodel <- randomForest(price~.,data = train, mtry = 6, ntree =300)
print(model1)
predictions <- predict(tmodel,test)
MAE(predictions,test[,1])
MAPE(predictions,test[,1])*100
RMSE(predictions,test[,1])
print(tmodel)
RMSE(predictions,test[,1])
sqrt(25944)
sqrt(25944.79)
MAPE(predictions,test[,1])*100
temp <- data.frame(speed=as.integer(25),hd=as.integer(170),ram=as.integer(4),screen=as.integer(15),cd=c("no"),multi=c("no"),premium=c("yes"),ads=as.integer(94),trend=as.integer(1), stringsAsFactors = TRUE)
model1$forest$xlevels$cd
levels(temp$cd) <- model1$forest$xlevels$cd
levels(temp$multi) <- model1$forest$xlevels$multi
levels(temp$premium) <-
d
model1$forest$xlevels$cd
levels(temp$cd) <- model1$forest$xlevels$cd
levels(temp$multi) <- model1$forest$xlevels$multi
levels(temp$premium) <- model1$forest$xlevels$premium
test[2,]
p <- predict(tmodel,temp)
p
MAE(predictions,test[,1])
